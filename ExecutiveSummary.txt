Stacey Newman
Moushumi Maria
Diana Gorun

Assignment Overview

The purpose of the fourth assignment is to build a fault tolerant version of the key value store using Paxos. It builds on the replicated server implementation of the third assignment. One of the biggest bottlenecks of the previous implementation was its inability to operate in case of a replica failure for any reasons. This poses a major bottleneck on systems having several replicas. Paxos based implementation will no longer require every replica to succeed to enable the system as a whole to serve its client. The new implementation will succeed if consensus is achieved among replica servers based on majority. As long as at least half of the servers agree and can execute an operation, the operation will succeed. We will replicate our key value store across five distinct servers as before. All five servers will be running the same server code. In implementing this change, our client code remained identical to assignment three.

Replication will still remain in use. If all five replicas agree and can execute an operation then the state of all replicas will update in the same way as in the previous assignment. Given the small number of replicas, and the short duration of test/evaluation, they may not fail due to the reasons real servers fail in the production systems. In order to demonstrate fault tolerance, the failures will be somewhat simulated either by probabilistically injecting them or by forcing timeouts. Client will communicate to server using IP address of any of the replicas. To enable communication among replicas, the server code will read the IP/hostnames of other replicas from a local file called 'replica'.

Technical Impression

Part of the implementation time for the fourth project was spent for bringing every required thing together from last three projects. This effort brought back in multithreading and mutual exclusion on top of the implementation of assignment three.

The RPC protocol is extended to add four new functions. The first couple of new functions implement prepare phase for put and delete commands. The following two functions implement execution phase of these commands. We first developed such a RPC specification during assignment two. Subsequently we extended it once in assignment three and for the second time for this assignment. Over the last three assignments client code remained identical. This is reflected in RPC specification as well. First three functions in the specification, that are required for the client, remained the same. This observation states that we could create two such specifications: one for client server communication; and the other for server-server communication, i.e. the part that evolved as we moved from 2PC to Paxos based implementation. Doing so would be good from encapsulation point of view: in case the specifications need to be shared among server and client implementation/user teams, the details of server side RPC calls may need to be kept hidden from that of client.

In the implementation, both proposer and acceptor roles are executed in their own threads. To keep the implementations similar across many operations, several threads are created. In many cases, a new thread is created and the creating (i.e. old) thread is immediately asked to wait for the new thread to complete its execution first before the old thread could proceed (i.e. the old thread called join on the new thread). This effectively limited the power of parallelism. This was not the primary concern of our implementation. But it remains as an opportunity for us to revisit and improve the implementation so that it uses multithreads effectively.

Much of the difficulty of working with multiple machines (e.g. five servers in our case) was experienced, understood, and dealt with during assignment three. That experience helped us avoid losing any extra time than was minimally required for setting up and evaluate the new implementation on the cluster.

One of the intellectually challenging questions that we are confronted with is about how to deal with failures in the right way and recover from that while a protocol such as Paxos is in use. Think majority allows the system to go ahead on executing an operation across replicas; that indicates some replicas are likely getting out of sync. How to bring them back in sync again?
